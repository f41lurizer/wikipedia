Author: Mazin Jindeel
Date: 5/15
Functionality: Find the most interconnected wikipedia pages

Instructions:

Note: each step may take a while, depending on your system's limitations.  

1: Download dataset
    download the wikipedia dataset in XML. 
    You can find the download link here
    http://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia
    Note: you will need at about 12GB free on your disk to complete this step. 

2: uncompress the data. 
    Note: uncompressed, the dataset expands to about 50GB, so you will need at least that much space free on your disk. 

3: run parseScript.sh to extract just titles and links 
    type 'bash parseScript.sh arg1 arg2'
    The first argument should be the uncompressed file
    The second argument is wherever you want the output to be located
    Note: dataset should shrink to ~10GB, so have at least that much space free. 

4: remove namespace links  and create the database to be used by making and running createDatabase
    Many "links" are not actually linking to other pages, but separate namespaces
    We need to remove these faux "links", since they don't link to actual pages and will degrade performance if not removed
    steps to make and run removeForbidden:
        A. type 'make createForbidden' to make the utility
        B. type './removeForbidden arg1 ' to run the utility
            arg1 is the path of your input file
        C. by default, an sqlite database called adj.db will be created in the working directory
        Read the documentation in createDatabase.cpp to view all the available switches and how to use them.  

    Note: make sure you have enough space free on your disk to run this utility -- in the case that there are no namespaces, or if you choose to log forbidden pages, the total output (adj.db and the logfile) will be the size of the file created in step 3, so make sure you have at least that much space free. 
