#Find the most interconnected wikipedia pages
###Author: Mazin Jindeel
###Date: 5/15

#Instructions:

Note: These steps may take a while, as the data is quite large. 

1. Download dataset
    * download the wikipedia dataset in XML
      [here](http://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia). 
    * Note: you will need at about 12GB free on your disk to complete this step. 

2. Uncompress the data. 
	* `bunzip2 enwiki-20170820-pages-articles.xml.bz2`

3. run parseScript.sh to extract just titles and links.
    * `./ parseScript.sh enwiki-20180820-pages-articles.xml graph_input.txt`
    * output should shrink to ~10GB, so have at least that much space free. 
	* Links to namespaces are removed, because they are not considered pages

4. Analyze the data
	* This requires a great deal of RAM (over 16gb) or a ton of swap space.
	* Install the dependencies (we only need networkx), so pip3 install networkx
	* Run the command `./analysis.py graph_input.txt in.txt out.txt`
	* See files in.txt and out.txt for the output of the analysis.
